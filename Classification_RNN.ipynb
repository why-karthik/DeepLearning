{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EJJE2RGZL8qo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "seed = 11068 \n",
        "np.random.seed(seed) ## Use your own unique seed\n",
        "tf.random.set_seed(seed) ## Use your own unique seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IVzVDIR6arH"
      },
      "outputs": [],
      "source": [
        "## TODO\n",
        "## Load Data\n",
        "##for.npy files\n",
        "\n",
        "X_train = np.load(\"x_train.npy\").reshape(-1, 28*28)\n",
        "X_test = np.load(\"x_test.npy\").reshape(-1, 28*28)\n",
        "## Create Validation data from train splits\n",
        "X_val = None\n",
        "Y_train = np.load(\"y_train.npy\")\n",
        "Y_test = np.load(\"y_test.npy\")\n",
        "Y_val = None\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tYbjA5C_uFgO"
      },
      "outputs": [],
      "source": [
        "## TODO\n",
        "## Load Data\n",
        "## For .npz files\n",
        "\n",
        "#-----> This code is for loading .npz files <--------#\n",
        "\n",
        "X_train_data = np.load(\"train-imgs.npz\")\n",
        "X_train = [X_train_data[key] for key in X_train_data.keys()]\n",
        "X_train = np.concatenate(X_train, axis = 0).reshape(-1, 28*28)\n",
        "\n",
        "X_test_data = np.load(\"test-imgs.npz\")\n",
        "X_test = [X_test_data[key] for key in X_test_data.keys()]\n",
        "X_test = np.concatenate(X_test, axis = 0).reshape(-1, 28*28)\n",
        "\n",
        "## Create Validation data from train splits\n",
        "X_val = None\n",
        "Y_train_data = np.load(\"train-labels.npz\")\n",
        "Y_train = [Y_train_data[key] for key in Y_train_data.keys()]\n",
        "Y_train = np.concatenate(Y_train, axis = 0)\n",
        "\n",
        "Y_test_data = np.load(\"test-labels.npz\")\n",
        "Y_test = [Y_test_data[key] for key in Y_test_data.keys()]\n",
        "Y_test = np.concatenate(Y_test, axis = 0)\n",
        "\n",
        "Y_val = None\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "D32LRkMJGV2V"
      },
      "outputs": [],
      "source": [
        "## Define model specific parameters\n",
        "## Repalce None with values\n",
        "size_input = 28*28\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 256\n",
        "size_hidden3 = 128\n",
        "size_output = 10\n",
        "number_of_train_examples = 48000\n",
        "number_of_dev_examples = 12000\n",
        "number_of_test_examples = 10000\n",
        "batch_size = 1000\n",
        "NUM_EPOCHS = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qSrIHwVxRbYc"
      },
      "outputs": [],
      "source": [
        "## TODO\n",
        "# Normalize your X\n",
        "# Convert your labels into one-hot encoding\n",
        "# Hint look into function tf.keras.utils.to_categorical or Other function is tf.one_hot()\n",
        "\n",
        "def normalize_func(X):\n",
        "    X = (X - np.min(X))/(np.max(X) - np.min(X))\n",
        "    return X\n",
        "\n",
        "# Apply the normalization function to each example in the dataset\n",
        "X_train = normalize_func(X_train)\n",
        "X_val = normalize_func(X_val)\n",
        "X_test = normalize_func(X_test)\n",
        "\n",
        "\n",
        "Y_train = normalize_func(Y_train)\n",
        "Y_val = normalize_func(Y_val)\n",
        "Y_test = normalize_func(Y_test)\n",
        "\n",
        "# Convert your labels into one-hot encoding\n",
        "\n",
        "Y_train = tf.cast(Y_train, dtype=tf.int64)\n",
        "Y_val = tf.cast(Y_val, dtype=tf.int64)\n",
        "\n",
        "Y_train = tf.one_hot(Y_train, depth = 10)\n",
        "\n",
        "Y_val = tf.one_hot(Y_val, depth = 10)\n",
        "\n",
        "Y_test = tf.one_hot(Y_test, depth = 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZINz3raX0-Ig"
      },
      "source": [
        "For L1 Regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ycucHIu9OWWi"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    ### TODO\n",
        "    ### Declare all your variables\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1]))\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2]))\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3]))\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output]))\n",
        "\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "  \n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    ## TODO\n",
        "\n",
        "    # Convert your pred and true to tf.float32\n",
        "    # Ensure your shapes are (batch_size, size_output)\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    # This above function will add softmax to your final layer and then perform cross-entropy\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "\n",
        "    # TODO\n",
        "    # Modify above loss and add L1 regularization\n",
        "\n",
        "    l1_lamda = 0.001\n",
        "    l1_reg = l1_lamda * (tf.reduce_sum(tf.abs(self.W1)) + tf.reduce_sum(tf.abs(self.W2)) + tf.reduce_sum(tf.abs(self.W3)) + tf.reduce_sum(tf.abs(self.W4)))\n",
        "    loss = l1_reg + loss_x\n",
        "    return loss\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    lr = 0.001 #Play with your learning rate\n",
        "    optimizer = tf.keras.optimizers.RMSprop(learning_rate = lr) # Play with various Optimizers and pick the one that works best for your design\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables)) # Optimizer\n",
        "           \n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "\n",
        "    ## TODO\n",
        "    # Compute values in hidden layers\n",
        "    H1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    Z1 = tf.nn.sigmoid(H1)\n",
        "\n",
        "    H2 = tf.matmul(Z1, self.W2) + self.b2\n",
        "    Z2 = tf.nn.sigmoid(H2)\n",
        "\n",
        "    H3 = tf.matmul(Z2, self.W3) + self.b3\n",
        "    Z3 = tf.nn.sigmoid(H3)\n",
        "\n",
        "    H4 = tf.matmul(Z3, self.W4) + self.b4\n",
        "    # Compute output\n",
        "    output = H4 # Logits\n",
        "    \n",
        "    # Remember the loss function has keras loss objective, that explicitly applies softmax over logits\n",
        "    return (output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtak3Y0N1DNC"
      },
      "source": [
        " For L2 Regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-hkZVGk04xv"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    ### TODO\n",
        "    ### Declare all your variables\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1]))\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2]))\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3]))\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output]))\n",
        "\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "  \n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    ## TODO\n",
        "\n",
        "    # Convert your pred and true to tf.float32\n",
        "    # Ensure your shapes are (batch_size, size_output)\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    # This above function will add softmax to your final layer and then perform cross-entropy\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "\n",
        "    # TODO\n",
        "    # Modify above loss and add  L2 regularization\n",
        "\n",
        "    l2_lamda = 0.001\n",
        "    l2_reg = l2_lamda * (tf.reduce_sum(tf.square(self.W1)) + tf.reduce_sum(tf.square(self.W2)) + tf.reduce_sum(tf.square(self.W3)) + tf.reduce_sum(tf.square(self.W4)))\n",
        "    loss = l2_reg + loss_x\n",
        "    return loss\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    lr = 0.001 #Play with your learning rate\n",
        "    optimizer = tf.keras.optimizers.RMSprop(learning_rate = lr) # Play with various Optimizers and pick the one that works best for your design\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables)) # Optimizer\n",
        "           \n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "\n",
        "    ## TODO\n",
        "    # Compute values in hidden layers\n",
        "    H1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    Z1 = tf.nn.sigmoid(H1)\n",
        "\n",
        "    H2 = tf.matmul(Z1, self.W2) + self.b2\n",
        "    Z2 = tf.nn.sigmoid(H2)\n",
        "\n",
        "    H3 = tf.matmul(Z2, self.W3) + self.b3\n",
        "    Z3 = tf.nn.sigmoid(H3)\n",
        "\n",
        "    H4 = tf.matmul(Z3, self.W4) + self.b4\n",
        "    # Compute output\n",
        "    output = H4 # Logits\n",
        "    \n",
        "    # Remember the loss function has keras loss objective, that explicitly applies softmax over logits\n",
        "    return (output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDOs-cNLQbPw"
      },
      "outputs": [],
      "source": [
        "# Initialize model using GPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(25, seed=epoch*(seed)).batch(batch_size)\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_gpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_gpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    mlp_on_gpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_gpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  \n",
        "  ## TODO\n",
        "  cur_val_acc = 0.0\n",
        "  accuracy_z = 0.0\n",
        "\n",
        "  probs = mlp_on_gpu.forward(X_val)\n",
        "  ## Now calculate Validation Accuracy\n",
        "  probs = tf.nn.softmax(probs)\n",
        "  correct_prediction = tf.equal(tf.argmax(probs, 1), tf.argmax(Y_val, 1))\n",
        "  accuracy_z = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy_z.numpy()\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpZC6lpCRSe6"
      },
      "outputs": [],
      "source": [
        "## TODO - Write a inference code and perform only one pass through test data\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "test_loss_total = 0.0\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).shuffle(25, seed=epoch*(seed)).batch(batch_size)\n",
        "  \n",
        "  \n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_gpu.forward(inputs)\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  test_loss_total = test_loss_total + mlp_on_gpu.loss(preds, outputs)\n",
        "    \n",
        "\n",
        "print('Test MSE: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_train.shape[0]))\n",
        "\n",
        "probs = mlp_on_gpu.forward(X_test)\n",
        "probs = tf.nn.softmax(probs)\n",
        "correct_prediction = tf.equal(tf.argmax(probs, 1), tf.argmax(Y_test, 1))\n",
        "accuracy_z = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "\n",
        "print('\\nTesting Accuracy: {:.4f}'.format(accuracy_z))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
